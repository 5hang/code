docker 安装 zookeeper

docker-compose.yml 

version: "3.1"
services:
 zk:
  image: daocloud.io/daocloud/zookeeper:latest
  restart: always
  container_name: zk
  ports:
   - 2181:2181
docker-compose up -d    到这里成功安装并启动zookeeper

docker exec -it zk bash
cd bin
./zkCli.sh


下载 kafka
wget https://archive.apache.org/dist/kafka/2.4.1/kafka_2.11-2.4.1.tgz

解压kafka
config/server.properies 配置

#kafka集群中的唯一id
broker.id=0
#kafka 部署的服务的提供服务的IP端口
listeners=PLAINTEXT://192.168.13.134:9092
#kafka日志路径
log.dir=/tmp/kafka-logs
#kafka 链接 zookeeper的地址
zookeeper.connect=192.168.13.134:2181

启动kafka
[root@xxx bin]# ./kafka-server-start.sh -daemon ../config/server.properties 
查看是否启动成功了



************************************kafka中的概念************************

创建topic
./kafka-topics.sh --create --zookeeper 192.168.13.134:2181 --replication-factor 1 --partitions 1 --topic test

查看topic
./kafka-topics.sh  --list --zookeeper  192.168.13.134:2181

发送消息
./kafka-console-producer.sh --broker-list 192.168.13.134:9092  --topic test


消费消息
1:  从最后一条偏移量+1开始消费
 ./kafka-console-consumer.sh --bootstrap-server 192.168.13.134:9092 --topic test

2： 从头开始消费
./kafka-console-consumer.sh --bootstrap-server  192.168.13.134:9092 --from-beginning --topic test




**生产者将消息发送到broker ，broker将消息保存在本地日志文件中
/var/kafka-log 
消息的保存是有顺序的，
消费者可以通过offset来读取指定位置的消息











